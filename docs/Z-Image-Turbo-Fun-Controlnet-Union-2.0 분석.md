# Z-Image-Turbo-Fun-Controlnet-Union-2.0 ë¶„ì„

## 1. ê°œìš”

**Z-Image-Turbo-Fun-Controlnet-Union-2.0**ì€ VideoX-Fun í”„ë¡œì íŠ¸ì˜ ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ë¡œ, **ë‹¤ì¤‘ ì»¨íŠ¸ë¡¤ ì¡°ê±´**(Canny, HED, Depth, Pose, MLSD ë“±)ì„ ì§€ì›í•˜ëŠ” í†µí•© ControlNet ëª¨ë¸ì…ë‹ˆë‹¤.

---

## 2. ì§€ì›í•˜ëŠ” Control íƒ€ì…

Z-Image-Turbo-Fun-Controlnet-Union-2.0ì€ **6ê°€ì§€ Control íƒ€ì…**ì„ ì§€ì›í•©ë‹ˆë‹¤:

| Control íƒ€ì… | ì„¤ëª… | ìš©ë„ |
|--------------|------|------|
| **Canny** | ì´ë¯¸ì§€ì˜ ìœ¤ê³½ì„ /ì—£ì§€ë¥¼ ì¶”ì¶œ | êµ¬ì¡°ì  ì™¸ê³½ì„  ê¸°ë°˜ ìƒì„± |
| **HED** | Holistically-Nested Edge Detection, ì„¸ë°€í•œ ì—£ì§€ ê°ì§€ | ë” ë¶€ë“œëŸ¬ìš´ ìœ¤ê³½ì„  ê¸°ë°˜ ìƒì„± |
| **Depth** | ê¹Šì´ ë§µ (ê±°ë¦¬ ì •ë³´) | 3D ê³µê°„ê° ìˆëŠ” ì´ë¯¸ì§€ ìƒì„± |
| **Pose** | ì¸ì²´ ê´€ì ˆ ìœ„ì¹˜ ê°ì§€ (DWPose) | ìºë¦­í„° í¬ì¦ˆ ì œì–´ |
| **MLSD** | Multi-Line Segment Detection, ì§ì„  ê°ì§€ | ê±´ì¶•ë¬¼/êµ¬ì¡°ë¬¼ ê¸°ë°˜ ìƒì„± |
| **Inpaint** | ë§ˆìŠ¤í¬ ê¸°ë°˜ ì˜ì—­ ìˆ˜ì • (2.0 ì „ìš©) | ì´ë¯¸ì§€ ë¶€ë¶„ ìˆ˜ì •/í¸ì§‘ |

### Control ì´ë¯¸ì§€ ì˜ˆì‹œ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ì›ë³¸ ì´ë¯¸ì§€     â†’    Control ì¶”ì¶œ    â†’    ìƒˆ ì´ë¯¸ì§€ ìƒì„±              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  [ì‚¬ëŒ ì‚¬ì§„]    â†’    [Pose ìŠ¤ì¼ˆë ˆí†¤]  â†’    [ë‹¤ë¥¸ ìŠ¤íƒ€ì¼ì˜ ìºë¦­í„°]      â”‚
â”‚  [ê±´ë¬¼ ì‚¬ì§„]    â†’    [Canny ì—£ì§€]     â†’    [ê°™ì€ êµ¬ì¡°ì˜ ë‹¤ë¥¸ ê±´ë¬¼]     â”‚
â”‚  [í’ê²½ ì‚¬ì§„]    â†’    [Depth ë§µ]       â†’    [ê°™ì€ êµ¬ë„ì˜ ë‹¤ë¥¸ í’ê²½]     â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Control ì¶”ì¶œ ë° ì‚¬ìš© ì˜ˆì‹œ ì½”ë“œ

#### 1. Canny Edge Detection

```python
import cv2
import numpy as np
from PIL import Image

def extract_canny(image_path, low_threshold=100, high_threshold=200):
    """Canny ì—£ì§€ ì¶”ì¶œ"""
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    edges = cv2.Canny(gray, low_threshold, high_threshold)
    edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)
    return edges_colored

# ì‚¬ìš© ì˜ˆì‹œ
canny_image = extract_canny("input.jpg", low=100, high=200)
Image.fromarray(canny_image).save("canny_control.png")
```

#### 2. Depth Map (ZoeDepth ì‚¬ìš©)

```python
import torch
import cv2
import numpy as np
from einops import rearrange

# ZoeDepth ëª¨ë¸ ë¡œë“œ (VideoX-Fun ë‚´ì¥)
from comfyui.annotator.zoe.zoedepth.models.zoedepth.zoedepth_v1 import ZoeDepth
from comfyui.annotator.zoe.zoedepth.utils.config import get_config

def extract_depth(image_path, model_path="ZoeD_M12_N.pt"):
    """Depth ë§µ ì¶”ì¶œ (ZoeDepth)"""
    # ëª¨ë¸ ë¡œë“œ
    model = ZoeDepth.build_from_config(get_config("zoedepth", "infer"))
    model.load_state_dict(torch.load(model_path, map_location="cpu")['model'], strict=False)
    model = model.to("cuda").eval()
    
    # ì´ë¯¸ì§€ ì²˜ë¦¬
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image_tensor = torch.from_numpy(image).to("cuda", torch.float32) / 255.0
    image_tensor = rearrange(image_tensor, 'h w c -> 1 c h w')
    
    # Depth ì¶”ë¡ 
    with torch.no_grad():
        depth = model.infer(image_tensor)
        depth = depth[0, 0].cpu().numpy()
        
        # ì •ê·œí™”
        vmin, vmax = np.percentile(depth, 2), np.percentile(depth, 85)
        depth = (depth - vmin) / (vmax - vmin)
        depth = 1.0 - depth  # ë°˜ì „
        depth_image = (depth * 255).clip(0, 255).astype(np.uint8)
        depth_image = cv2.cvtColor(depth_image, cv2.COLOR_GRAY2RGB)
    
    return depth_image

# ì‚¬ìš© ì˜ˆì‹œ
depth_image = extract_depth("input.jpg")
Image.fromarray(depth_image).save("depth_control.png")
```

#### 3. Pose Detection (DWPose ì‚¬ìš©)

```python
import cv2
import numpy as np
from PIL import Image

# DWPose ëª¨ë¸ ë¡œë“œ (VideoX-Fun ë‚´ì¥)
from comfyui.annotator.dwpose_utils import DWposeDetector

def extract_pose(image_path, det_model="yolox_l.onnx", pose_model="dw-ll_ucoco_384.onnx"):
    """Pose ìŠ¤ì¼ˆë ˆí†¤ ì¶”ì¶œ (DWPose)"""
    # ëª¨ë¸ ë¡œë“œ
    detector = DWposeDetector(det_model, pose_model)
    
    # ì´ë¯¸ì§€ ì²˜ë¦¬
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    # Pose ì¶”ë¡ 
    pose_image = detector(image)
    
    return pose_image

# ì‚¬ìš© ì˜ˆì‹œ
pose_image = extract_pose("input.jpg")
Image.fromarray(pose_image).save("pose_control.png")
```

#### 4. HED (Holistically-Nested Edge Detection)

```python
import cv2
import numpy as np
from PIL import Image

def extract_hed(image_path, prototxt="deploy.prototxt", caffemodel="hed_pretrained_bsds.caffemodel"):
    """HED ì—£ì§€ ì¶”ì¶œ (OpenCV DNN)"""
    # HED ëª¨ë¸ ë¡œë“œ
    net = cv2.dnn.readNetFromCaffe(prototxt, caffemodel)
    
    # ì´ë¯¸ì§€ ë¡œë“œ
    image = cv2.imread(image_path)
    (H, W) = image.shape[:2]
    
    # Blob ìƒì„±
    blob = cv2.dnn.blobFromImage(
        image, 
        scalefactor=1.0, 
        size=(W, H),
        mean=(104.00698793, 116.66876762, 122.67891434),
        swapRB=False, 
        crop=False
    )
    
    # ì¶”ë¡ 
    net.setInput(blob)
    hed = net.forward()
    hed = cv2.resize(hed[0, 0], (W, H))
    hed = (255 * hed).astype('uint8')
    hed = cv2.cvtColor(hed, cv2.COLOR_GRAY2RGB)
    
    return hed

# ì‚¬ìš© ì˜ˆì‹œ (HED ëª¨ë¸ íŒŒì¼ í•„ìš”)
# hed_image = extract_hed("input.jpg")
# Image.fromarray(hed_image).save("hed_control.png")
```

#### 5. MLSD (Multi-Line Segment Detection)

```python
import cv2
import numpy as np
from PIL import Image

def extract_mlsd(image_path, score_thr=0.1, dist_thr=20.0):
    """MLSD ì§ì„  ê°ì§€ (OpenCV LSD)"""
    image = cv2.imread(image_path)
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    
    # LSD ë¼ì¸ ê°ì§€
    lsd = cv2.createLineSegmentDetector(0)
    lines, width, prec, nfa = lsd.detect(gray)
    
    # ê²°ê³¼ ì´ë¯¸ì§€ ìƒì„±
    result = np.zeros_like(image)
    if lines is not None:
        for line in lines:
            x0, y0, x1, y1 = map(int, line[0])
            cv2.line(result, (x0, y0), (x1, y1), (255, 255, 255), 1)
    
    return result

# ì‚¬ìš© ì˜ˆì‹œ
mlsd_image = extract_mlsd("input.jpg")
Image.fromarray(mlsd_image).save("mlsd_control.png")
```

#### 6. Inpainting (ë§ˆìŠ¤í¬ ê¸°ë°˜ í¸ì§‘) - 2.0 ì „ìš©

```python
import torch
from PIL import Image
import numpy as np

# ë§ˆìŠ¤í¬ ì´ë¯¸ì§€ ìƒì„± ì˜ˆì‹œ
def create_mask(image_size, mask_region):
    """ë§ˆìŠ¤í¬ ì´ë¯¸ì§€ ìƒì„± (í°ìƒ‰=ìˆ˜ì •í•  ì˜ì—­)"""
    mask = np.zeros((image_size[0], image_size[1]), dtype=np.uint8)
    x, y, w, h = mask_region  # (x, y, width, height)
    mask[y:y+h, x:x+w] = 255
    return mask

# Inpainting ì „ì²´ íŒŒì´í”„ë¼ì¸ (2.0 ì „ìš©)
from videox_fun.utils.utils import get_image_latent

sample_size = [1728, 992]

# ì›ë³¸ ì´ë¯¸ì§€, ë§ˆìŠ¤í¬, ì»¨íŠ¸ë¡¤ ì´ë¯¸ì§€ ë¡œë“œ
inpaint_image = get_image_latent("original.png", sample_size=sample_size)[:, :, 0]
mask_image = get_image_latent("mask.png", sample_size=sample_size)[:, :1, 0]
control_image = get_image_latent("pose.png", sample_size=sample_size)[:, :, 0]

# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
result = pipeline(
    prompt="ìƒˆë¡œìš´ ë‚´ìš© ì„¤ëª…",
    height=sample_size[0],
    width=sample_size[1],
    image=inpaint_image,           # ì›ë³¸ ì´ë¯¸ì§€
    mask_image=mask_image,         # ë§ˆìŠ¤í¬ (í°ìƒ‰=ìˆ˜ì • ì˜ì—­)
    control_image=control_image,   # ì»¨íŠ¸ë¡¤ ì´ë¯¸ì§€
    num_inference_steps=25,
    control_context_scale=0.75,
).images
```

### í•„ìš”í•œ Annotator ëª¨ë¸ íŒŒì¼

| ëª¨ë¸ | ë‹¤ìš´ë¡œë“œ ê²½ë¡œ |
|------|---------------|
| **ZoeDepth** | `ZoeD_M12_N.pt` - [HuggingFace](https://huggingface.co/lllyasviel/Annotators/resolve/main/ZoeD_M12_N.pt) |
| **DWPose (Det)** | `yolox_l.onnx` - [HuggingFace](https://huggingface.co/yzd-v/DWPose/resolve/main/yolox_l.onnx) |
| **DWPose (Pose)** | `dw-ll_ucoco_384.onnx` - [HuggingFace](https://huggingface.co/yzd-v/DWPose/resolve/main/dw-ll_ucoco_384.onnx) |
| **HED** | `hed_pretrained_bsds.caffemodel` - [GitHub](https://github.com/s9xie/hed) |

### ì£¼ìš” íŠ¹ì§•

- **Union ëª¨ë¸**: í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ì—¬ëŸ¬ Control íƒ€ì…ì„ ëª¨ë‘ ì²˜ë¦¬ (ë³„ë„ ëª¨ë¸ ë¶ˆí•„ìš”)
- **í•™ìŠµ ë°ì´í„°**: 100ë§Œ ì¥ì˜ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ë¡œ 10,000 ìŠ¤í… í•™ìŠµ
- **í•™ìŠµ í•´ìƒë„**: 1328 (BFloat16 ì •ë°€ë„)
- **ê¶Œì¥ control_context_scale**: 0.65 ~ 0.80

---

## 3. í•µì‹¬ êµ¬ì„± ìš”ì†Œ

| êµ¬ì„± ìš”ì†Œ | ì„¤ëª… |
|-----------|------|
| **ZImageControlTransformer2DModel** | Control ê¸°ëŠ¥ì´ ì¶”ê°€ëœ Transformer ëª¨ë¸ |
| **ZImageControlPipeline** | ì´ë¯¸ì§€ ìƒì„± íŒŒì´í”„ë¼ì¸ |
| **Qwen3ForCausalLM** | í…ìŠ¤íŠ¸ ì¸ì½”ë” |
| **AutoencoderKL** | VAE ëª¨ë¸ |
| **FlowMatchEulerDiscreteScheduler** | ìŠ¤ì¼€ì¤„ëŸ¬ |

---

## 4. 2.0 ë²„ì „ ì„¤ì •

### ì„¤ì • íŒŒì¼ (`z_image_control_2.0.yaml`)

```yaml
transformer_additional_kwargs:
    control_layers_places: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28]
    control_refiner_layers_places: [0, 1]
    add_control_noise_refiner: true   # Noise Refinerì—ì„œë„ Control ì ìš©
    control_in_dim: 33                # 33ì±„ë„ ì…ë ¥ (16 latent + 1 mask + 16 control)
```

### 2.0 í•µì‹¬ íŠ¹ì§•

| íŠ¹ì§• | ì„¤ëª… |
|------|------|
| **Noise Refiner** | Refiner ë‹¨ê³„ì—ì„œë„ Control ì ìš© |
| **Inpainting ì§€ì›** | ë§ˆìŠ¤í¬ ê¸°ë°˜ ì´ë¯¸ì§€ í¸ì§‘ ê°€ëŠ¥ |
| **15ê°œ Control Layers** | ë” ì •êµí•œ Control ì ìš© |
| **ê¶Œì¥ ìŠ¤í… ìˆ˜** | 25 ìŠ¤í… |

---

## 5. Python ì‚¬ìš© ì½”ë“œ (ë‹¨ìˆœí™” ë²„ì „)

```python
import torch
from diffusers import FlowMatchEulerDiscreteScheduler
from omegaconf import OmegaConf
from PIL import Image

# VideoX-Fun ëª¨ë“ˆ import
from videox_fun.dist import set_multi_gpus_devices
from videox_fun.models import (AutoencoderKL, AutoTokenizer,
                               Qwen3ForCausalLM, ZImageControlTransformer2DModel)
from videox_fun.pipeline import ZImageControlPipeline
from videox_fun.utils.utils import get_image_latent

# ================== ì„¤ì • ==================
config_path = "config/z_image/z_image_control_2.0.yaml"
model_name = "models/Diffusion_Transformer/Z-Image-Turbo/"
transformer_path = "models/Personalized_Model/Z-Image-Turbo-Fun-Controlnet-Union-2.0.safetensors"

weight_dtype = torch.bfloat16  # RTX 30/40 ì‹œë¦¬ì¦ˆ ì‚¬ìš©ì‹œ
# weight_dtype = torch.float16  # RTX 20 ì‹œë¦¬ì¦ˆ ì´í•˜

# ================== ëª¨ë¸ ë¡œë“œ ==================
device = set_multi_gpus_devices(1, 1)
config = OmegaConf.load(config_path)

# 1. Transformer ë¡œë“œ
transformer = ZImageControlTransformer2DModel.from_pretrained(
    model_name, 
    subfolder="transformer",
    low_cpu_mem_usage=True,
    torch_dtype=weight_dtype,
    transformer_additional_kwargs=OmegaConf.to_container(config['transformer_additional_kwargs']),
).to(weight_dtype)

# 2. ControlNet Union ê°€ì¤‘ì¹˜ ë¡œë“œ
from safetensors.torch import load_file
state_dict = load_file(transformer_path)
m, u = transformer.load_state_dict(state_dict, strict=False)
print(f"missing keys: {len(m)}, unexpected keys: {len(u)}")

# 3. VAE ë¡œë“œ
vae = AutoencoderKL.from_pretrained(model_name, subfolder="vae").to(weight_dtype)

# 4. í…ìŠ¤íŠ¸ ì¸ì½”ë” ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(model_name, subfolder="tokenizer")
text_encoder = Qwen3ForCausalLM.from_pretrained(
    model_name, subfolder="text_encoder", 
    torch_dtype=weight_dtype,
    low_cpu_mem_usage=True,
)

# 5. ìŠ¤ì¼€ì¤„ëŸ¬
scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(model_name, subfolder="scheduler")

# 6. íŒŒì´í”„ë¼ì¸ êµ¬ì„±
pipeline = ZImageControlPipeline(
    vae=vae,
    tokenizer=tokenizer,
    text_encoder=text_encoder,
    transformer=transformer,
    scheduler=scheduler,
)
pipeline.enable_model_cpu_offload(device=device)

# ================== ì´ë¯¸ì§€ ìƒì„± ==================
sample_size = [1728, 992]  # [height, width]
control_image = get_image_latent("asset/pose.jpg", sample_size=sample_size)[:, :, 0]

prompt = "A beautiful woman with purple hair on the beach"
seed = 43
generator = torch.Generator(device=device).manual_seed(seed)

with torch.no_grad():
    result = pipeline(
        prompt=prompt, 
        negative_prompt=" ",
        height=sample_size[0],
        width=sample_size[1],
        generator=generator,
        guidance_scale=0.0,          # 0.0 = ê°€ì´ë˜ìŠ¤ ë¹„í™œì„±í™” (Turbo ëª¨ë¸ íŠ¹ì„±)
        control_image=control_image,
        num_inference_steps=25,       # 2.0ì€ 25 ìŠ¤í… ê¶Œì¥
        control_context_scale=0.75,   # ì»¨íŠ¸ë¡¤ ê°•ë„ (0.65~0.80 ê¶Œì¥)
    ).images

# ì €ì¥
result[0].save("output.png")
```

---

## 6. GPU ë©”ëª¨ë¦¬ ëª¨ë“œ ë° ì–‘ìí™” ì˜µì…˜

### 6.1 GPU ë©”ëª¨ë¦¬ ëª¨ë“œ

| ëª¨ë“œ | ì†ë„ | VRAM ì‚¬ìš©ëŸ‰ | ì„¤ëª… |
|------|------|-------------|------|
| `model_full_load` | âš¡ ê°€ì¥ ë¹ ë¦„ | ê°€ì¥ ë§ìŒ | ì „ì²´ ëª¨ë¸ GPU ë¡œë“œ |
| `model_full_load_and_qfloat8` | ë¹ ë¦„ | ì¤‘ê°„ | GPU ë¡œë“œ + FP8 ì–‘ìí™” |
| `model_cpu_offload` | ë³´í†µ | ì ìŒ | ì‚¬ìš© í›„ CPUë¡œ ì˜¤í”„ë¡œë“œ |
| `model_cpu_offload_and_qfloat8` | ë³´í†µ | ë” ì ìŒ | CPU ì˜¤í”„ë¡œë“œ + FP8 ì–‘ìí™” |
| `sequential_cpu_offload` | ğŸ¢ ê°€ì¥ ëŠë¦¼ | ìµœì†Œ | ë ˆì´ì–´ë³„ CPU ì˜¤í”„ë¡œë“œ |

### 6.2 FP8 ì–‘ìí™” ì‚¬ìš©ë²•

FP8 (`torch.float8_e4m3fn`) ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ë©´ **VRAMì„ ì•½ 50% ì ˆì•½**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from videox_fun.utils.fp8_optimization import (
    convert_model_weight_to_float8,
    convert_weight_dtype_wrapper
)

# ================== FP8 ì–‘ìí™” ì ìš© ì˜ˆì‹œ ==================

GPU_memory_mode = "model_cpu_offload_and_qfloat8"  # ê¶Œì¥

# íŒŒì´í”„ë¼ì¸ ìƒì„± í›„ ë©”ëª¨ë¦¬ ëª¨ë“œ ì ìš©
if GPU_memory_mode == "sequential_cpu_offload":
    pipeline.enable_sequential_cpu_offload(device=device)

elif GPU_memory_mode == "model_cpu_offload_and_qfloat8":
    # FP8 ì–‘ìí™” ì ìš© (ì œì™¸í•  ëª¨ë“ˆ ì§€ì •)
    convert_model_weight_to_float8(
        transformer, 
        exclude_module_name=["img_in", "txt_in", "timestep"],  # ì…ì¶œë ¥ ë ˆì´ì–´ ì œì™¸
        device=device
    )
    convert_weight_dtype_wrapper(transformer, weight_dtype)
    pipeline.enable_model_cpu_offload(device=device)

elif GPU_memory_mode == "model_cpu_offload":
    pipeline.enable_model_cpu_offload(device=device)

elif GPU_memory_mode == "model_full_load_and_qfloat8":
    convert_model_weight_to_float8(
        transformer, 
        exclude_module_name=["img_in", "txt_in", "timestep"],
        device=device
    )
    convert_weight_dtype_wrapper(transformer, weight_dtype)
    pipeline.to(device=device)

else:  # model_full_load
    pipeline.to(device=device)
```

### 6.3 FP8 ì–‘ìí™” ì£¼ì˜ì‚¬í•­

| í•­ëª© | ì„¤ëª… |
|------|------|
| **ì œì™¸ ëª¨ë“ˆ** | `img_in`, `txt_in`, `timestep` ë“± ì…ì¶œë ¥ ë ˆì´ì–´ëŠ” ì–‘ìí™”ì—ì„œ ì œì™¸ |
| **í’ˆì§ˆ ì˜í–¥** | ì•½ê°„ì˜ í’ˆì§ˆ ì €í•˜ ê°€ëŠ¥ (ëŒ€ë¶€ë¶„ ë¬´ì‹œí•  ìˆ˜ì¤€) |
| **í˜¸í™˜ì„±** | RTX 40 ì‹œë¦¬ì¦ˆ ì´ìƒì—ì„œ ìµœì  ì„±ëŠ¥ (FP8 í•˜ë“œì›¨ì–´ ì§€ì›) |
| **dtype ë˜í¼** | `convert_weight_dtype_wrapper`ë¡œ forward ì‹œ ìë™ dtype ë³€í™˜ |

---

## 7. ìƒ˜í”ŒëŸ¬ (Scheduler) ì˜µì…˜

### ì§€ì› ìƒ˜í”ŒëŸ¬

| ìƒ˜í”ŒëŸ¬ | í´ë˜ìŠ¤ | íŠ¹ì§• |
|--------|--------|------|
| `Flow` | `FlowMatchEulerDiscreteScheduler` | ê¸°ë³¸ ìƒ˜í”ŒëŸ¬, ì•ˆì •ì  |
| `Flow_Unipc` | `FlowUniPCMultistepScheduler` | ë” ë¹ ë¥¸ ìˆ˜ë ´, ì ì€ ìŠ¤í… |
| `Flow_DPM++` | `FlowDPMSolverMultistepScheduler` | ê³ í’ˆì§ˆ ê²°ê³¼ |

### ìƒ˜í”ŒëŸ¬ ì„ íƒ ì½”ë“œ

```python
from diffusers import FlowMatchEulerDiscreteScheduler
from videox_fun.utils.fm_solvers import FlowDPMSolverMultistepScheduler
from videox_fun.utils.fm_solvers_unipc import FlowUniPCMultistepScheduler

# ìƒ˜í”ŒëŸ¬ ì„ íƒ
sampler_name = "Flow"  # "Flow", "Flow_Unipc", "Flow_DPM++" ì¤‘ ì„ íƒ

scheduler_dict = {
    "Flow": FlowMatchEulerDiscreteScheduler,
    "Flow_Unipc": FlowUniPCMultistepScheduler,
    "Flow_DPM++": FlowDPMSolverMultistepScheduler,
}

Chosen_Scheduler = scheduler_dict[sampler_name]
scheduler = Chosen_Scheduler.from_pretrained(model_name, subfolder="scheduler")
```

### ê¶Œì¥ ì„¤ì •

| ìƒ˜í”ŒëŸ¬ | ê¶Œì¥ ìŠ¤í… ìˆ˜ | ìš©ë„ |
|--------|-------------|------|
| `Flow` | 25 | ê¸°ë³¸, ì•ˆì •ì ì¸ ê²°ê³¼ |
| `Flow_Unipc` | 15~20 | ë¹ ë¥¸ ìƒì„± |
| `Flow_DPM++` | 20~25 | ê³ í’ˆì§ˆ ì¶œë ¥ |

---

## 8. LoRA ì‚¬ìš©ë²•

### LoRA ì ìš©/í•´ì œ

```python
from videox_fun.utils.lora_utils import merge_lora, unmerge_lora

# LoRA ê²½ë¡œ ë° ê°€ì¤‘ì¹˜
lora_path = "models/Lora/your_lora.safetensors"
lora_weight = 0.8  # 0.0 ~ 1.0

# LoRA ì ìš© (ëª¨ë¸ì— ë³‘í•©)
pipeline = merge_lora(
    pipeline, 
    lora_path, 
    lora_weight, 
    device=device, 
    dtype=weight_dtype
)

# ì´ë¯¸ì§€ ìƒì„±...
result = pipeline(prompt=prompt, ...).images

# LoRA í•´ì œ (ì›ë³¸ ë³µì›)
pipeline = unmerge_lora(
    pipeline, 
    lora_path, 
    lora_weight, 
    device=device, 
    dtype=weight_dtype
)
```

### LoRA íŠ¹ì§•

- **ë™ì  ë¡œë”©**: ëŸ°íƒ€ì„ì— LoRAë¥¼ ì ìš©/í•´ì œ ê°€ëŠ¥
- **ê°€ì¤‘ì¹˜ ì¡°ì ˆ**: `lora_weight`ë¡œ LoRA ì˜í–¥ë„ ì¡°ì ˆ (0.0=ì—†ìŒ, 1.0=100%)
- **ë‹¤ì¤‘ LoRA**: ì—¬ëŸ¬ LoRAë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì ìš© ê°€ëŠ¥

---

## 9. Inpainting ì‚¬ìš©ë²• (2.0 ì „ìš©)

```python
# Inpaintingì— í•„ìš”í•œ ì¶”ê°€ ì…ë ¥
inpaint_image = get_image_latent("asset/8.png", sample_size=sample_size)[:, :, 0]
mask_image = get_image_latent("asset/mask.png", sample_size=sample_size)[:, :1, 0]

result = pipeline(
    prompt=prompt,
    height=sample_size[0],
    width=sample_size[1],
    image=inpaint_image,           # ì›ë³¸ ì´ë¯¸ì§€
    mask_image=mask_image,         # ë§ˆìŠ¤í¬ (í°ìƒ‰=ìˆ˜ì •í•  ì˜ì—­)
    control_image=control_image,   # ì»¨íŠ¸ë¡¤ ì´ë¯¸ì§€ (Pose, Canny ë“±)
    num_inference_steps=25,
    control_context_scale=0.75,
).images
```

---

## 10. í•µì‹¬ íŒŒë¼ë¯¸í„° ì •ë¦¬

### ê¸°ë³¸ íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ë²”ìœ„/íƒ€ì… | ê¸°ë³¸ê°’ | ì„¤ëª… |
|----------|-----------|--------|------|
| `prompt` | str / List[str] | - | ìƒì„±í•  ì´ë¯¸ì§€ ì„¤ëª… |
| `negative_prompt` | str / List[str] | None | í”¼í•  ë‚´ìš© (TurboëŠ” " " ê¶Œì¥) |
| `height` | int | 1024 | ì¶œë ¥ ë†’ì´ (16 ë°°ìˆ˜) |
| `width` | int | 1024 | ì¶œë ¥ ë„ˆë¹„ (16 ë°°ìˆ˜) |
| `num_inference_steps` | int | 50 | ì¶”ë¡  ìŠ¤í… ìˆ˜ |
| `guidance_scale` | float | 5.0 | CFG ìŠ¤ì¼€ì¼ (**TurboëŠ” 0.0**) |
| `seed` | int | - | ëœë¤ ì‹œë“œ |

### Control ê´€ë ¨ íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ë²”ìœ„/íƒ€ì… | ê¸°ë³¸ê°’ | ì„¤ëª… |
|----------|-----------|--------|------|
| `control_image` | torch.FloatTensor | None | ì»¨íŠ¸ë¡¤ ì´ë¯¸ì§€ (Pose, Canny ë“±) |
| `control_context_scale` | 0.0~1.0 | 1.0 | ì»¨íŠ¸ë¡¤ ê°•ë„ (**0.65~0.80 ê¶Œì¥**) |
| `image` | torch.FloatTensor | None | Inpaintìš© ì›ë³¸ ì´ë¯¸ì§€ |
| `mask_image` | torch.FloatTensor | None | Inpaintìš© ë§ˆìŠ¤í¬ |

### ê³ ê¸‰ íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ë²”ìœ„/íƒ€ì… | ê¸°ë³¸ê°’ | ì„¤ëª… |
|----------|-----------|--------|------|
| `sigmas` | List[float] | None | ì»¤ìŠ¤í…€ ì‹œê·¸ë§ˆ ìŠ¤ì¼€ì¤„ |
| `cfg_normalization` | bool | False | CFG ì •ê·œí™” |
| `cfg_truncation` | float | 1.0 | CFG ì˜ë¼ë‚´ê¸° ë¹„ìœ¨ |
| `max_sequence_length` | int | 512 | ìµœëŒ€ í† í° ê¸¸ì´ |
| `num_images_per_prompt` | int | 1 | í”„ë¡¬í”„íŠ¸ë‹¹ ì´ë¯¸ì§€ ìˆ˜ |
| `output_type` | str | "pil" | ì¶œë ¥ íƒ€ì… ("pil", "latent") |

### ê¶Œì¥ ì„¤ì •

| ì„¤ì • | ê¶Œì¥ê°’ |
|------|--------|
| `num_inference_steps` | **25** |
| `control_context_scale` | **0.65~0.80** |
| `guidance_scale` | **0.0** (Turbo ëª¨ë¸) |
| `sample_size` | **[1728, 992]** (í•™ìŠµ í•´ìƒë„) |

---

## 11. í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬

### CUDA ì§€ì› GPU ì‚¬ìš© ì‹œ (ê¶Œì¥)

```bash
# PyTorch CUDA ë²„ì „ ì„¤ì¹˜ (CUDA 12.6 ê¸°ì¤€)
pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu126

# ë‚˜ë¨¸ì§€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install diffusers>=0.30.1 transformers>=4.46.2 safetensors omegaconf Pillow
```

> âš ï¸ **ì£¼ì˜**: ì¼ë°˜ `pip install torch`ëŠ” CPU ë²„ì „ì´ ì„¤ì¹˜ë©ë‹ˆë‹¤. GPU ê°€ì†ì„ ìœ„í•´ì„œëŠ” ë°˜ë“œì‹œ ìœ„ ëª…ë ¹ì–´ë¡œ CUDA ë²„ì „ì„ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.


### ì „ì²´ requirements.txt

```txt
# í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
torch>=2.1.2
torchvision
diffusers>=0.30.1
transformers>=4.46.2
safetensors
omegaconf
Pillow
accelerate>=0.25.0

# ì¶”ê°€ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
einops              # í…ì„œ ì—°ì‚° (Depth ì¶”ì¶œ ë“±)
opencv-python       # ì´ë¯¸ì§€ ì²˜ë¦¬ (Canny, MLSD ë“±)
onnxruntime         # Pose ì¶”ì¶œ (DWPose)
numpy               # ìˆ˜ì¹˜ ì—°ì‚°
scikit-image        # ì´ë¯¸ì§€ ì²˜ë¦¬

# ì„ íƒ ë¼ì´ë¸ŒëŸ¬ë¦¬
gradio>=3.41.2      # WebUI ì‚¬ìš© ì‹œ
decord              # ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œ
imageio[ffmpeg]     # ë¹„ë””ì˜¤ ì €ì¥ ì‹œ
```

---

## 12. ì•„í‚¤í…ì²˜ íë¦„ë„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ZImageControlPipeline                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  1. Text Encoding (Qwen3ForCausalLM)                       â”‚
â”‚     prompt â†’ text_embeds                                    â”‚
â”‚                                                             â”‚
â”‚  2. Control Image Processing                                â”‚
â”‚     control_image â†’ VAE.encode â†’ control_latents           â”‚
â”‚                                                             â”‚
â”‚  3. Noise Refiner (2.0 ì‹ ê·œ)                               â”‚
â”‚     control_noise_refiner â†’ refiner_hints                  â”‚
â”‚                                                             â”‚
â”‚  4. Main Transformer Blocks (30 layers)                    â”‚
â”‚     latents + hints â†’ denoised_latents                     â”‚
â”‚                                                             â”‚
â”‚  5. VAE Decode                                             â”‚
â”‚     latents â†’ output_image                                 â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 13. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ

> âš ï¸ **ì¤‘ìš”**: Z-Image-Turbo-Fun-Controlnet-Union-2.0ì„ ì‚¬ìš©í•˜ë ¤ë©´ **ë‘ ëª¨ë¸ì´ ëª¨ë‘ í•„ìš”í•©ë‹ˆë‹¤!**

### í•„ìš”í•œ ëª¨ë¸

| ëª¨ë¸ | ì œê³µí•˜ëŠ” ê²ƒ | í¬ê¸° |
|------|-------------|------|
| **Base ëª¨ë¸ (Z-Image-Turbo)** | Transformer ê¸°ë³¸ êµ¬ì¡°, VAE, Tokenizer, Text Encoder, Scheduler | ~**26GB** (ì „ì²´ í´ë”) |
| **ControlNet Union 2.0** | Control ê´€ë ¨ ë ˆì´ì–´ ê°€ì¤‘ì¹˜ | ~**3.1GB** (ë‹¨ì¼ `.safetensors`) |

### Annotator ëª¨ë¸ (Control ì¶”ì¶œìš©, ì„ íƒ)

| ëª¨ë¸ | ìš©ë„ | í¬ê¸° |
|------|------|------|
| **ZoeD_M12_N.pt** | Depth ë§µ ì¶”ì¶œ | ~**1.44GB** |
| **yolox_l.onnx** | Pose - ì‚¬ëŒ ê°ì§€ | ~**217MB** |
| **dw-ll_ucoco_384.onnx** | Pose - ê´€ì ˆ ì¶”ì¶œ | ~**134MB** |
| **Canny** | ìœ¤ê³½ì„  ì¶”ì¶œ | ë³„ë„ ëª¨ë¸ ë¶ˆí•„ìš” (OpenCV ë‚´ì¥) |
| **MLSD** | ì§ì„  ê°ì§€ | ë³„ë„ ëª¨ë¸ ë¶ˆí•„ìš” (OpenCV ë‚´ì¥) |

### ì´ í•„ìš” ìš©ëŸ‰

| êµ¬ì„± | ìš©ëŸ‰ |
|------|------|
| **ìµœì†Œ (Base + ControlNet)** | ~**29GB** |
| **ì „ì²´ (+ Annotator ëª¨ë¸)** | ~**31GB** |

### ë‹¤ìš´ë¡œë“œ ë§í¬

- **Base ëª¨ë¸**: [Z-Image-Turbo (HuggingFace)](https://huggingface.co/alibaba-pai/Z-Image-Turbo)
- **ControlNet Union 2.0**: [Z-Image-Turbo-Fun-Controlnet-Union (HuggingFace)](https://huggingface.co/alibaba-pai/Z-Image-Turbo-Fun-Controlnet-Union)

### ë¡œë“œ ìˆœì„œ

```python
# 1ë‹¨ê³„: Base ëª¨ë¸ì—ì„œ ê¸°ë³¸ êµ¬ì¡° ë¡œë“œ
transformer = ZImageControlTransformer2DModel.from_pretrained(
    "models/Diffusion_Transformer/Z-Image-Turbo/",
    subfolder="transformer",
    ...
)

# 2ë‹¨ê³„: ControlNet Union 2.0 ê°€ì¤‘ì¹˜ë¥¼ ì¶”ê°€ë¡œ ë¡œë“œ
state_dict = load_file("models/Personalized_Model/Z-Image-Turbo-Fun-Controlnet-Union-2.0.safetensors")
transformer.load_state_dict(state_dict, strict=False)
```

### ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
models/
â”œâ”€â”€ Diffusion_Transformer/
â”‚   â””â”€â”€ Z-Image-Turbo/
â”‚       â”œâ”€â”€ transformer/
â”‚       â”œâ”€â”€ vae/
â”‚       â”œâ”€â”€ tokenizer/
â”‚       â”œâ”€â”€ text_encoder/
â”‚       â””â”€â”€ scheduler/
â””â”€â”€ Personalized_Model/
    â””â”€â”€ Z-Image-Turbo-Fun-Controlnet-Union-2.0.safetensors
```

---

## 14. ê´€ë ¨ íŒŒì¼ ê²½ë¡œ (VideoX-Fun)

### ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸

| íŒŒì¼ | ì„¤ëª… |
|------|------|
| `examples/z_image_fun/predict_t2i_control_2.0.py` | **Control T2I** (Text-to-Image + Control) |
| `examples/z_image_fun/predict_i2i_inpaint_2.0.py` | **Inpainting** (ì´ë¯¸ì§€ í¸ì§‘ + Control) |

### ëª¨ë¸/íŒŒì´í”„ë¼ì¸ êµ¬í˜„

| íŒŒì¼ | ì„¤ëª… |
|------|------|
| `videox_fun/pipeline/pipeline_z_image_control.py` | Control íŒŒì´í”„ë¼ì¸ (ZImageControlPipeline) |
| `videox_fun/models/z_image_transformer2d_control.py` | Control Transformer (ZImageControlTransformer2DModel) |

### ì„¤ì • íŒŒì¼

| íŒŒì¼ | ì„¤ëª… |
|------|------|
| `config/z_image/z_image_control_2.0.yaml` | 2.0 ë²„ì „ ì„¤ì • |

### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

| ëª¨ë¸ | íŒŒì¼ëª… |
|------|--------|
| **Base ëª¨ë¸** | `Z-Image-Turbo/` í´ë” ì „ì²´ (~26GB) |
| **ControlNet 2.0** | `Z-Image-Turbo-Fun-Controlnet-Union-2.0.safetensors` (~3.1GB) |

---

## 15. ê³ ê¸‰ ì˜µì…˜ (Z-Image ì „ìš©)

> **ì°¸ê³ **: `predict_t2i_control_2.0.py` ì˜ˆì œì—ì„œ ì§€ì›í•˜ëŠ” ì˜µì…˜ë“¤ì…ë‹ˆë‹¤.

### 15.1 ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°

#### ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ë²”ìœ„/ì˜µì…˜ | ê¸°ë³¸ê°’ | ì„¤ëª… |
|----------|-----------|--------|------|
| `width` | 64~2048 (step 16) | 992 | ì¶œë ¥ ë„ˆë¹„ |
| `height` | 64~2048 (step 16) | 1728 | ì¶œë ¥ ë†’ì´ |
| `seed` | 0~2^64 | 43 | ëœë¤ ì‹œë“œ |
| `num_inference_steps` | 1~200 | 25 | ì¶”ë¡  ìŠ¤í… ìˆ˜ |
| `guidance_scale` | 0.0~20.0 | **0.0** | ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼ (Turbo=0) |
| `control_context_scale` | 0.0~1.0 | 0.75 | ì»¨íŠ¸ë¡¤ ê°•ë„ (0.65~0.80 ê¶Œì¥) |

#### LoRA ì˜µì…˜

| íŒŒë¼ë¯¸í„° | ë²”ìœ„/ì˜µì…˜ | ê¸°ë³¸ê°’ | ì„¤ëª… |
|----------|-----------|--------|------|
| `lora_path` | íŒŒì¼ ê²½ë¡œ | None | LoRA íŒŒì¼ |
| `lora_weight` | 0.0~1.0 | 0.55 | LoRA ê°€ì¤‘ì¹˜ |

#### Control ì¶”ì¶œ ì˜µì…˜

| íƒ€ì… | íŒŒë¼ë¯¸í„° | ë²”ìœ„ | ì„¤ëª… |
|------|----------|------|------|
| **Canny** | `low_threshold` | 0~255 (ê¸°ë³¸ 100) | í•˜í•œ ì„ê³„ê°’ |
|           | `high_threshold` | 0~255 (ê¸°ë³¸ 200) | ìƒí•œ ì„ê³„ê°’ |
| **Depth** | - | - | ZoeDepth ëª¨ë¸ ì‚¬ìš© |
| **Pose** | - | - | DWPose ëª¨ë¸ ì‚¬ìš© |

### 15.2 Multi-GPU / ë¶„ì‚° ì²˜ë¦¬

```python
from videox_fun.dist import set_multi_gpus_devices, shard_model

# Multi-GPU ì„¤ì •
ulysses_degree = 1      # Ulysses ë¶„ì‚° ì°¨ìˆ˜
ring_degree = 1         # Ring ë¶„ì‚° ì°¨ìˆ˜
# ì°¸ê³ : ulysses_degree Ã— ring_degree = GPU ê°œìˆ˜

device = set_multi_gpus_devices(ulysses_degree, ring_degree)

# FSDP (Fully Sharded Data Parallel) - ëŒ€ê·œëª¨ GPUì—ì„œ ë©”ëª¨ë¦¬ ì ˆì•½
fsdp_dit = False        # Transformer FSDP í™œì„±í™”
fsdp_text_encoder = False  # Text Encoder FSDP í™œì„±í™”

if ulysses_degree > 1 or ring_degree > 1:
    transformer.enable_multi_gpus_inference()
    if fsdp_dit:
        from functools import partial
        shard_fn = partial(shard_model, device_id=device, param_dtype=weight_dtype, 
                          module_to_wrapper=list(transformer.transformer_blocks))
        pipeline.transformer = shard_fn(pipeline.transformer)
```

### 15.3 torch.compile ìµœì í™”

ê³ ì • í•´ìƒë„ì—ì„œ **ì†ë„ í–¥ìƒ** (ì²« ì‹¤í–‰ ì‹œ ì»´íŒŒì¼ ì‹œê°„ í•„ìš”):

```python
compile_dit = True

if compile_dit:
    for i in range(len(pipeline.transformer.transformer_blocks)):
        pipeline.transformer.transformer_blocks[i] = torch.compile(
            pipeline.transformer.transformer_blocks[i]
        )
    print("Add Compile")
```

> âš ï¸ **ì£¼ì˜**: `sequential_cpu_offload`ì™€ í˜¸í™˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

### 15.4 Attention íƒ€ì… ì„ íƒ

```python
import os

# Attention ë°±ì—”ë“œ ì„ íƒ (pipeline ìƒì„± ì „ì— ì„¤ì •)
os.environ['VIDEOX_ATTENTION_TYPE'] = "FLASH_ATTENTION"

# ì˜µì…˜:
# - "FLASH_ATTENTION": Flash Attention 2 (ê¸°ë³¸, ê°€ì¥ ë¹ ë¦„)
# - "SAGE_ATTENTION": Sage Attention
# - "TORCH_SCALED_DOT": PyTorch ê¸°ë³¸ Scaled Dot Product
```

### 15.5 Inpainting ì˜ˆì œ (2.0 ì „ìš©)

> 2.0 ì „ìš© ê¸°ëŠ¥: Inpainting + Control ë™ì‹œ ì‚¬ìš©

```python
import torch
from diffusers import FlowMatchEulerDiscreteScheduler
from omegaconf import OmegaConf

from videox_fun.dist import set_multi_gpus_devices
from videox_fun.models import (AutoencoderKL, AutoTokenizer,
                               Qwen3ForCausalLM, ZImageControlTransformer2DModel)
from videox_fun.pipeline import ZImageControlPipeline
from videox_fun.utils.utils import get_image_latent

# ================== 2.0 ë²„ì „ ì„¤ì • ==================
config_path = "config/z_image/z_image_control_2.0.yaml"  # 2.0 ì„¤ì •
model_name = "models/Diffusion_Transformer/Z-Image-Turbo/"
transformer_path = "models/Personalized_Model/Z-Image-Turbo-Fun-Controlnet-Union-2.0.safetensors"  # 2.0 ëª¨ë¸

weight_dtype = torch.bfloat16
sample_size = [1728, 992]

# Inpainting ì…ë ¥
control_image_path = "asset/pose.jpg"
inpaint_image_path = "asset/8.png"      # ì›ë³¸ ì´ë¯¸ì§€
mask_image_path = "asset/mask.png"       # ë§ˆìŠ¤í¬ (í°ìƒ‰=ìˆ˜ì • ì˜ì—­)

prompt = "A beautiful woman with purple hair on the beach"
seed = 43
num_inference_steps = 25   # 2.0ì€ 25 ìŠ¤í… ê¶Œì¥
guidance_scale = 0.0
control_context_scale = 0.75

# ================== ëª¨ë¸ ë¡œë“œ ==================
device = set_multi_gpus_devices(1, 1)
config = OmegaConf.load(config_path)

transformer = ZImageControlTransformer2DModel.from_pretrained(
    model_name, subfolder="transformer",
    low_cpu_mem_usage=True, torch_dtype=weight_dtype,
    transformer_additional_kwargs=OmegaConf.to_container(config['transformer_additional_kwargs']),
).to(weight_dtype)

from safetensors.torch import load_file
state_dict = load_file(transformer_path)
transformer.load_state_dict(state_dict, strict=False)

vae = AutoencoderKL.from_pretrained(model_name, subfolder="vae").to(weight_dtype)
tokenizer = AutoTokenizer.from_pretrained(model_name, subfolder="tokenizer")
text_encoder = Qwen3ForCausalLM.from_pretrained(
    model_name, subfolder="text_encoder", torch_dtype=weight_dtype, low_cpu_mem_usage=True
)
scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(model_name, subfolder="scheduler")

pipeline = ZImageControlPipeline(
    vae=vae, tokenizer=tokenizer, text_encoder=text_encoder,
    transformer=transformer, scheduler=scheduler,
)
pipeline.enable_model_cpu_offload(device=device)

# ================== ì´ë¯¸ì§€ ë¡œë“œ ==================
control_image = get_image_latent(control_image_path, sample_size=sample_size)[:, :, 0]
inpaint_image = get_image_latent(inpaint_image_path, sample_size=sample_size)[:, :, 0]
mask_image = get_image_latent(mask_image_path, sample_size=sample_size)[:, :1, 0]

# ================== Inpainting ì‹¤í–‰ ==================
generator = torch.Generator(device=device).manual_seed(seed)

with torch.no_grad():
    result = pipeline(
        prompt=prompt,
        negative_prompt=" ",
        height=sample_size[0],
        width=sample_size[1],
        generator=generator,
        guidance_scale=guidance_scale,
        image=inpaint_image,              # ì›ë³¸ ì´ë¯¸ì§€
        mask_image=mask_image,            # ë§ˆìŠ¤í¬
        control_image=control_image,      # ì»¨íŠ¸ë¡¤ ì´ë¯¸ì§€
        num_inference_steps=num_inference_steps,
        control_context_scale=control_context_scale,
    ).images

result[0].save("output_inpaint_2.0.png")
```

### 15.6 ì¢…í•© Python ì˜ˆì œ (ëª¨ë“  ì˜µì…˜ í¬í•¨)

```python
import os
import torch
from diffusers import FlowMatchEulerDiscreteScheduler
from omegaconf import OmegaConf

from videox_fun.dist import set_multi_gpus_devices
from videox_fun.models import (AutoencoderKL, AutoTokenizer,
                               Qwen3ForCausalLM, ZImageControlTransformer2DModel)
from videox_fun.pipeline import ZImageControlPipeline
from videox_fun.utils.fp8_optimization import (convert_model_weight_to_float8,
                                               convert_weight_dtype_wrapper)
from videox_fun.utils.lora_utils import merge_lora, unmerge_lora
from videox_fun.utils.utils import get_image_latent

# ================== í™˜ê²½ ì„¤ì • ==================
os.environ['VIDEOX_ATTENTION_TYPE'] = "FLASH_ATTENTION"

# ================== íŒŒë¼ë¯¸í„° ì„¤ì • ==================
config_path = "config/z_image/z_image_control_2.0.yaml"
model_name = "models/Diffusion_Transformer/Z-Image-Turbo/"
transformer_path = "models/Personalized_Model/Z-Image-Turbo-Fun-Controlnet-Union-2.0.safetensors"

# ë©”ëª¨ë¦¬ & ì„±ëŠ¥ ì˜µì…˜
GPU_memory_mode = "model_cpu_offload_and_qfloat8"
weight_dtype = torch.bfloat16
compile_dit = False

# ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°
sample_size = [1728, 992]  # [height, width]
prompt = "A beautiful woman with purple hair on the beach"
negative_prompt = " "
seed = 43
num_inference_steps = 25
guidance_scale = 0.0        # Turbo ëª¨ë¸ì€ 0 ì‚¬ìš©
control_context_scale = 0.75

# LoRA ì˜µì…˜
lora_path = None
lora_weight = 0.55

# ================== ëª¨ë¸ ë¡œë“œ ==================
device = set_multi_gpus_devices(1, 1)
config = OmegaConf.load(config_path)

# Transformer
transformer = ZImageControlTransformer2DModel.from_pretrained(
    model_name, subfolder="transformer",
    low_cpu_mem_usage=True, torch_dtype=weight_dtype,
    transformer_additional_kwargs=OmegaConf.to_container(config['transformer_additional_kwargs']),
).to(weight_dtype)

# ControlNet ê°€ì¤‘ì¹˜ ë¡œë“œ
from safetensors.torch import load_file
state_dict = load_file(transformer_path)
transformer.load_state_dict(state_dict, strict=False)

# VAE, Tokenizer, Text Encoder
vae = AutoencoderKL.from_pretrained(model_name, subfolder="vae").to(weight_dtype)
tokenizer = AutoTokenizer.from_pretrained(model_name, subfolder="tokenizer")
text_encoder = Qwen3ForCausalLM.from_pretrained(
    model_name, subfolder="text_encoder", torch_dtype=weight_dtype, low_cpu_mem_usage=True
)
scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(model_name, subfolder="scheduler")

# íŒŒì´í”„ë¼ì¸ êµ¬ì„±
pipeline = ZImageControlPipeline(
    vae=vae, tokenizer=tokenizer, text_encoder=text_encoder,
    transformer=transformer, scheduler=scheduler,
)

# Compile (ì„ íƒ)
if compile_dit:
    for i in range(len(pipeline.transformer.transformer_blocks)):
        pipeline.transformer.transformer_blocks[i] = torch.compile(
            pipeline.transformer.transformer_blocks[i]
        )

# ë©”ëª¨ë¦¬ ëª¨ë“œ ì ìš©
if GPU_memory_mode == "model_cpu_offload_and_qfloat8":
    convert_model_weight_to_float8(transformer, exclude_module_name=["img_in", "txt_in", "timestep"], device=device)
    convert_weight_dtype_wrapper(transformer, weight_dtype)
    pipeline.enable_model_cpu_offload(device=device)
elif GPU_memory_mode == "model_cpu_offload":
    pipeline.enable_model_cpu_offload(device=device)
else:
    pipeline.to(device=device)

# LoRA ì ìš©
if lora_path:
    pipeline = merge_lora(pipeline, lora_path, lora_weight, device=device, dtype=weight_dtype)

# ================== ì´ë¯¸ì§€ ìƒì„± ==================
generator = torch.Generator(device=device).manual_seed(seed)
control_image = get_image_latent("asset/pose.jpg", sample_size=sample_size)[:, :, 0]

with torch.no_grad():
    result = pipeline(
        prompt=prompt,
        negative_prompt=negative_prompt,
        height=sample_size[0],
        width=sample_size[1],
        generator=generator,
        guidance_scale=guidance_scale,
        control_image=control_image,
        num_inference_steps=num_inference_steps,
        control_context_scale=control_context_scale,
    ).images

# ì €ì¥
result[0].save("output.png")

# ì •ë¦¬
if lora_path:
    pipeline = unmerge_lora(pipeline, lora_path, lora_weight, device=device, dtype=weight_dtype)
```
